{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWNmaTcfqNEsoNva+8fTY7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["📌 Overview\n","This project implements a Part-of-Speech (POS) tagger using a Hidden Markov Model (HMM), enhanced with suffix-based rules and word-level features to handle unknown words effectively. The model is trained on the combined Treebank and Brown corpora from the NLTK library, utilizing the Universal POS tagset.​\n","\n","🧱 Key Components\n","1. Data Preparation\n","Corpus Loading: The code loads tagged sentences from the Treebank and Brown corpora, both annotated with the Universal POS tagset.​\n","\n","Data Splitting: The combined dataset is split into training and testing sets using an 80-20 split to evaluate the model's performance.​\n","\n","2. EnhancedHMMTagger Class\n","This class encapsulates the HMM-based POS tagging logic with enhancements for better handling of unknown words.​\n","\n","a. Frequency Computation\n","Word Frequencies: Calculates the frequency of each word in the training set.​\n","\n","Tag Frequencies: Computes how often each POS tag appears.​\n","Medium\n","\n","Word-Tag Frequencies: Determines how often each word is associated with each tag.​\n","\n","b. Transition Probabilities\n","Tag Bigrams: Counts occurrences of consecutive tag pairs to understand tag transitions.​\n","\n","Transition Matrix: Constructs a matrix representing the probability of transitioning from one tag to another, applying Laplace smoothing to handle unseen transitions.​\n","\n","c. Start Probabilities\n","Sentence Start Tags: Calculates the probability distribution of tags that begin sentences, again using Laplace smoothing.​\n","\n","d. Suffix Rules for Unknown Words\n","Suffix-Based Tagging: Defines a set of suffixes (e.g., 'ing', 'ed', 'ly') associated with probable tags and their weights to guess the tag of unknown words.​\n","\n","e. Emission Probabilities\n","Known Words: For words seen during training, computes the probability of a word given a tag.​\n","\n","Unknown Words: For unseen words, applies suffix rules and word features (e.g., capitalization, digits) to estimate emission probabilities.​\n","\n","f. Viterbi Algorithm for Decoding\n","Initialization: Sets up the initial probabilities for the first word in the sentence.​\n","\n","Recursion: Iteratively computes the most probable tag sequence for the sentence using dynamic programming.​\n","FreeCodeCamp\n","+1\n","Seong Hyun Hwang\n","+1\n","\n","Termination: Backtracks through the computed probabilities to determine the optimal sequence of tags.​\n","\n","🧪 Testing and Evaluation\n","Sample Sentences: The model is tested on a set of predefined sentences to demonstrate its tagging capabilities.​\n","\n","Accuracy Evaluation: The model's performance is evaluated on a sample of 100 sentences from the test set, calculating the percentage of correctly predicted tags.​\n","\n","📈 Performance Insights\n","Handling Unknown Words: The integration of suffix rules and word features enhances the model's ability to tag words not seen during training.​\n","\n","Transition and Emission Probabilities: The use of Laplace smoothing ensures that the model can handle unseen tag transitions and word-tag combinations.​\n","\n","Viterbi Algorithm: Employing the Viterbi algorithm allows for efficient computation of the most probable tag sequence for a given sentence.​\n","\n","\n"],"metadata":{"id":"TfAs9nChY_EN"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt_tab')\n","import numpy as np\n","import pandas as pd\n","import math\n","from collections import defaultdict\n","from nltk.corpus import treebank, brown\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","\n","# Download and prepare data\n","nltk.download(['treebank', 'brown', 'universal_tagset', 'punkt'])\n","\n","# Combine Treebank and Brown corpora\n","tagged_sentences = list(treebank.tagged_sents(tagset='universal')) + \\\n","                   list(brown.tagged_sents(tagset='universal'))\n","\n","# Split into train and test\n","train_set, test_set = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n","\n","class EnhancedHMMTagger:\n","    def __init__(self, train_set):\n","        self.train_set = train_set\n","        self.train_tagged_words = [pair for sent in train_set for pair in sent]\n","        self.setup_frequencies()\n","        self.setup_transition_matrix()\n","        self.setup_start_probs()\n","        self.setup_suffix_rules()\n","\n","    def setup_frequencies(self):\n","        self.word_freq = defaultdict(int)\n","        self.tag_freq = defaultdict(int)\n","        self.word_tag_freq = defaultdict(lambda: defaultdict(int))\n","\n","        for word, tag in self.train_tagged_words:\n","            word_lower = word.lower()\n","            self.word_freq[word_lower] += 1\n","            self.tag_freq[tag] += 1\n","            self.word_tag_freq[word_lower][tag] += 1\n","\n","        self.vocab = set(self.word_freq.keys())\n","        self.tags = sorted(self.tag_freq.keys())\n","\n","    def setup_transition_matrix(self):\n","        self.tag_bigrams = defaultdict(lambda: defaultdict(int))\n","        for sentence in self.train_set:\n","            sentence_tags = [tag for _, tag in sentence]\n","            for i in range(len(sentence_tags)-1):\n","                current_tag = sentence_tags[i]\n","                next_tag = sentence_tags[i+1]\n","                self.tag_bigrams[current_tag][next_tag] += 1\n","\n","        self.transition_matrix = pd.DataFrame(0.0, index=self.tags, columns=self.tags)\n","        for t1 in self.tags:\n","            total = sum(self.tag_bigrams[t1].values())\n","            for t2 in self.tags:\n","                self.transition_matrix.loc[t1, t2] = \\\n","                    (self.tag_bigrams[t1].get(t2, 0) + 1) / (total + len(self.tags))\n","\n","    def setup_start_probs(self):\n","        self.start_counts = defaultdict(int)\n","        for sentence in self.train_set:\n","            if sentence:\n","                self.start_counts[sentence[0][1]] += 1\n","\n","        total_starts = sum(self.start_counts.values())\n","        self.start_probs = {tag: (self.start_counts.get(tag, 0) + 1) / \\\n","                          (total_starts + len(self.tags)) for tag in self.tags}\n","\n","    def setup_suffix_rules(self):\n","        # Enhanced suffix rules with weights\n","        self.suffix_rules = {\n","            'ing': [('VERB', 0.9), ('NOUN', 0.1)],\n","            'ed': [('VERB', 0.85), ('ADJ', 0.15)],\n","            'ly': [('ADV', 0.95)],\n","            'ment': [('NOUN', 0.98)],\n","            's': [('NOUN', 0.8), ('VERB', 0.2)],\n","            'ion': [('NOUN', 0.95)],\n","            'able': [('ADJ', 0.9)],\n","            'ive': [('ADJ', 0.9)],\n","            'est': [('ADJ', 0.95)]\n","        }\n","        self.sorted_suffixes = sorted(self.suffix_rules.keys(), key=len, reverse=True)\n","\n","    def emission_prob(self, word, tag):\n","        word_lower = word.lower()\n","        if word_lower in self.word_tag_freq:\n","            return (self.word_tag_freq[word_lower].get(tag, 0) + 1) / \\\n","                   (self.tag_freq[tag] + len(self.vocab))\n","        else:\n","            prob = self.handle_unknown_word(word, tag)\n","            return prob if prob > 0 else 1e-10\n","\n","    def handle_unknown_word(self, word, tag):\n","        for suffix in self.sorted_suffixes:\n","            if word.lower().endswith(suffix):\n","                for rule_tag, prob in self.suffix_rules[suffix]:\n","                    if tag == rule_tag:\n","                        return prob\n","                return 0.0\n","\n","        features = {\n","            'is_title': word.istitle(),\n","            'is_upper': word.isupper(),\n","            'is_digit': word.isdigit(),\n","            'has_hyphen': '-' in word,\n","            'is_numeric': any(c.isdigit() for c in word)\n","        }\n","\n","        if features['is_digit']:\n","            return 0.95 if tag == 'NUM' else 0.05/(len(self.tags)-1)\n","        elif features['is_title'] and len(word) > 3:\n","            return 0.8 if tag == 'NOUN' else 0.2/(len(self.tags)-1)\n","        elif features['is_upper']:\n","            return 0.7 if tag == 'NOUN' else 0.3/(len(self.tags)-1)\n","        else:\n","            return 0.5 if tag == 'NOUN' else 0.5/(len(self.tags)-1)\n","\n","    def viterbi_tag(self, sentence):\n","        words = [word for word in sentence]\n","        n = len(words)\n","        m = len(self.tags)\n","\n","        viterbi = np.zeros((n, m))\n","        backpointer = np.zeros((n, m), dtype=int)\n","\n","        # Initialization\n","        for j, tag in enumerate(self.tags):\n","            viterbi[0][j] = math.log(self.start_probs[tag] + 1e-10) + \\\n","                           math.log(self.emission_prob(words[0], tag) + 1e-10)\n","            backpointer[0][j] = -1\n","\n","        # Recursion\n","        for t in range(1, n):\n","            for j, current_tag in enumerate(self.tags):\n","                max_prob = -math.inf\n","                best_prev = 0\n","                for i, prev_tag in enumerate(self.tags):\n","                    prob = viterbi[t-1][i] + \\\n","                          math.log(self.transition_matrix.loc[prev_tag, current_tag] + 1e-10)\n","                    if prob > max_prob:\n","                        max_prob = prob\n","                        best_prev = i\n","\n","                viterbi[t][j] = max_prob + math.log(self.emission_prob(words[t], current_tag) + 1e-10)\n","                backpointer[t][j] = best_prev\n","\n","        # Termination\n","        best_last = np.argmax(viterbi[-1])\n","        best_path = [best_last]\n","        for t in range(n-1, 0, -1):\n","            best_last = backpointer[t][best_last]\n","            best_path.insert(0, best_last)\n","\n","        return list(zip(words, [self.tags[idx] for idx in best_path]))\n","\n","# Initialize and test the final tagger\n","final_tagger = EnhancedHMMTagger(train_set)\n","\n","test_sentences = [\n","    \"The quick brown fox jumps over the lazy dog .\",\n","    \"Natural language processing is a fascinating field of study .\",\n","    \"I would like to order two pizzas with extra cheese .\",\n","    \"Machine learning algorithms can improve over time .\"\n","]\n","\n","print(\"Final HMM POS Tagger Results:\")\n","for sent in test_sentences:\n","    words = word_tokenize(sent)\n","    tagged = final_tagger.viterbi_tag(words)\n","    print(\"\\nSentence:\", sent)\n","    print(\"POS Tags:\", tagged)\n","\n","# Evaluation\n","correct = total = 0\n","for sent in test_set[:100]:  # Evaluate on a sample\n","    words, gold_tags = zip(*sent)\n","    predicted = final_tagger.viterbi_tag(words)\n","    predicted_tags = [tag for _, tag in predicted]\n","    correct += sum(p == g for p, g in zip(predicted_tags, gold_tags))\n","    total += len(gold_tags)\n","print(\"\\nAccuracy on Test Set (Sample of 100 sentences): {:.2f}%\".format(100 * correct / total))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VCeaAS_EB8L","executionInfo":{"status":"ok","timestamp":1746111108517,"user_tz":-330,"elapsed":12814,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"1c0d6b42-0338-4fee-c92f-b531548721a8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Final HMM POS Tagger Results:\n","\n","Sentence: The quick brown fox jumps over the lazy dog .\n","POS Tags: [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'NOUN'), ('fox', 'NOUN'), ('jumps', 'NOUN'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', '.')]\n","\n","Sentence: Natural language processing is a fascinating field of study .\n","POS Tags: [('Natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'VERB'), ('is', 'VERB'), ('a', 'DET'), ('fascinating', 'ADJ'), ('field', 'NOUN'), ('of', 'ADP'), ('study', 'NOUN'), ('.', '.')]\n","\n","Sentence: I would like to order two pizzas with extra cheese .\n","POS Tags: [('I', 'PRON'), ('would', 'VERB'), ('like', 'VERB'), ('to', 'ADP'), ('order', 'NOUN'), ('two', 'NUM'), ('pizzas', 'NOUN'), ('with', 'ADP'), ('extra', 'ADJ'), ('cheese', 'NOUN'), ('.', '.')]\n","\n","Sentence: Machine learning algorithms can improve over time .\n","POS Tags: [('Machine', 'NOUN'), ('learning', 'VERB'), ('algorithms', 'NOUN'), ('can', 'VERB'), ('improve', 'VERB'), ('over', 'ADP'), ('time', 'NOUN'), ('.', '.')]\n","\n","Accuracy on Test Set (Sample of 100 sentences): 94.25%\n"]}]},{"cell_type":"markdown","source":["📌 Overview\n","This project implements a Machine Learning-based Part-of-Speech (POS) tagger using Logistic Regression. It relies on a set of contextual and morphological features extracted from words and their surrounding context. The model is trained on the Treebank and Brown corpora with Universal POS tags, then evaluated on unseen data and new sentences.\n","\n","🧱 Key Components\n","1. Data Preparation\n","Corpus Loading:\n","\n","Combines tagged sentences from the treebank and brown corpora using the 'universal' POS tagset.\n","\n","Ensures a diverse, balanced dataset.\n","\n","Train-Test Split:\n","\n","Randomly splits the data into 80% training and 20% test using train_test_split().\n","\n","2. Feature Engineering: word2features()\n","For each word in a sentence, this function extracts a rich set of features:\n","\n","Word-level features:\n","\n","Lowercased word\n","\n","Whether it's uppercase, title-cased, or a digit\n","\n","Last 2 and 3 letters (suffixes for morphological cues)\n","\n","Contextual features:\n","\n","Lowercased and title-cased versions of the previous and next words\n","\n","Special markers for beginning (BOS) or end (EOS) of a sentence\n","\n","These features help the model learn context-sensitive patterns.\n","\n","3. Training the Classifier\n","Vectorization:\n","\n","Converts feature dictionaries into numeric format using DictVectorizer, which performs one-hot encoding.\n","\n","Model:\n","\n","Trains a Logistic Regression classifier (sklearn.linear_model.LogisticRegression) to predict tags based on these features.\n","\n","max_iter=200 ensures convergence for large datasets.\n","\n","4. Evaluation\n","Accuracy Check:\n","\n","Tests on 100 randomly selected sentences from the test set.\n","\n","Reports the model’s accuracy using accuracy_score().\n","\n","5. Prediction on New Sentences\n","predict_tags():\n","\n","Tokenizes input sentences.\n","\n","Applies word2features() and uses the trained classifier to predict tags.\n","\n","Returns word-tag pairs for easy interpretation.\n","\n","📈 Sample Results\n","Example predictions are shown for:\n","\n","\"The quick brown fox jumps over the lazy dog .\"\n","\n","\"I would like to order two pizzas with extra cheese .\"\n","\n","The results demonstrate the model's ability to generalize to unseen sentences with good accuracy.\n","\n","✅ Strengths\n","Context-aware: Uses neighboring words as features.\n","\n","Morphological info: Leverages suffixes for better handling of unknown words.\n","\n","Simple yet powerful: Logistic Regression is fast and interpretable.\n","\n","⚠️ Limitations\n","Independent tag prediction: Predicts tags word-by-word, without sequence-level optimization like in HMM or CRF.\n","\n","No post-processing: Errors in early predictions don't influence future decisions.\n"],"metadata":{"id":"CqsVL6kWZMus"}},{"cell_type":"code","source":["import nltk\n","import random\n","import pandas as pd\n","from nltk.corpus import treebank, brown\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction import DictVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","nltk.download(['treebank', 'brown', 'universal_tagset'])\n","\n","# Load data\n","tagged_sentences = list(treebank.tagged_sents(tagset='universal')) + \\\n","                   list(brown.tagged_sents(tagset='universal'))\n","\n","train_sents, test_sents = train_test_split(tagged_sentences, test_size=0.2, random_state=42)\n","\n","# ---------- Feature Engineering ----------\n","def word2features(sent, i):\n","    word = sent[i][0]\n","    features = {\n","        'word.lower()': word.lower(),\n","        'word.isupper()': word.isupper(),\n","        'word.istitle()': word.istitle(),\n","        'word.isdigit()': word.isdigit(),\n","        'suffix-3': word[-3:],\n","        'suffix-2': word[-2:],\n","    }\n","    if i > 0:\n","        prev = sent[i-1][0]\n","        features.update({\n","            '-1:word.lower()': prev.lower(),\n","            '-1:word.istitle()': prev.istitle(),\n","        })\n","    else:\n","        features['BOS'] = True\n","\n","    if i < len(sent) - 1:\n","        nxt = sent[i+1][0]\n","        features.update({\n","            '+1:word.lower()': nxt.lower(),\n","            '+1:word.istitle()': nxt.istitle(),\n","        })\n","    else:\n","        features['EOS'] = True\n","    return features\n","\n","# ---------- Prepare Training Data ----------\n","X, y = [], []\n","for sent in train_sents:\n","    for i in range(len(sent)):\n","        feats = word2features(sent, i)\n","        X.append(feats)\n","        y.append(sent[i][1])\n","\n","# Vectorize\n","vec = DictVectorizer(sparse=True)\n","X_vectorized = vec.fit_transform(X)\n","\n","# Train\n","clf = LogisticRegression(max_iter=200)\n","clf.fit(X_vectorized, y)\n","\n","# ---------- Evaluation ----------\n","def predict_tags(sentence):\n","    feats = [word2features(list(zip(sentence, [None]*len(sentence))), i) for i in range(len(sentence))]\n","    feats_vec = vec.transform(feats)\n","    return list(zip(sentence, clf.predict(feats_vec)))\n","\n","# Test Accuracy\n","X_test, y_test = [], []\n","for sent in test_sents[:100]:  # Sample\n","    for i in range(len(sent)):\n","        X_test.append(word2features(sent, i))\n","        y_test.append(sent[i][1])\n","X_test_vec = vec.transform(X_test)\n","y_pred = clf.predict(X_test_vec)\n","\n","print(\"Accuracy on test set (100 sentences):\", round(accuracy_score(y_test, y_pred) * 100, 2), \"%\")\n","\n","# ---------- Try New Sentences ----------\n","test_sentences = [\n","    \"The quick brown fox jumps over the lazy dog .\",\n","    \"I would like to order two pizzas with extra cheese .\"\n","]\n","print(\"\\nML-Based POS Tagger Results:\")\n","for sent in test_sentences:\n","    words = nltk.word_tokenize(sent)\n","    tagged = predict_tags(words)\n","    print(\"\\nSentence:\", sent)\n","    print(\"POS Tags:\", tagged)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkxABFkEU_FZ","executionInfo":{"status":"ok","timestamp":1746111712325,"user_tz":-330,"elapsed":213064,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"605049a5-8b51-416f-9bf5-574d5c868712"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n","[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Package brown is already up-to-date!\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy on test set (100 sentences): 97.48 %\n","\n","ML-Based POS Tagger Results:\n","\n","Sentence: The quick brown fox jumps over the lazy dog .\n","POS Tags: [('The', np.str_('DET')), ('quick', np.str_('ADJ')), ('brown', np.str_('NOUN')), ('fox', np.str_('NOUN')), ('jumps', np.str_('NOUN')), ('over', np.str_('ADP')), ('the', np.str_('DET')), ('lazy', np.str_('ADJ')), ('dog', np.str_('NOUN')), ('.', np.str_('.'))]\n","\n","Sentence: I would like to order two pizzas with extra cheese .\n","POS Tags: [('I', np.str_('PRON')), ('would', np.str_('VERB')), ('like', np.str_('VERB')), ('to', np.str_('PRT')), ('order', np.str_('VERB')), ('two', np.str_('NUM')), ('pizzas', np.str_('NOUN')), ('with', np.str_('ADP')), ('extra', np.str_('ADJ')), ('cheese', np.str_('NOUN')), ('.', np.str_('.'))]\n"]}]},{"cell_type":"markdown","source":["📌 Overview\n","This script builds a Part-of-Speech (POS) tagger using a Conditional Random Field (CRF) model via the sklearn-crfsuite library. CRFs are well-suited for sequence labeling tasks like POS tagging because they consider the dependencies between adjacent tags (unlike logistic regression).\n","\n","🧱 Breakdown of the Code\n","1. Data Preparation\n","Dataset: Uses the Penn Treebank corpus via nltk.corpus.treebank.\n","\n","Splitting: First 3000 sentences for training, the rest for testing.\n","\n","2. Feature Engineering\n","Each word is converted into a feature dictionary using word2features(), which includes:\n","\n","Word-level features:\n","\n","Lowercase form, suffixes ([-2:], [-3:]), digit check, title case, uppercase\n","\n","Contextual features:\n","\n","Same features for the previous and next words\n","\n","BOS/EOS flags to indicate sentence boundaries\n","\n","These features are built for all words in each sentence using sent2features().\n","\n","3. Training the CRF Model\n","A CRF model is instantiated using:\n","\n","algorithm='lbfgs' (quasi-Newton optimization)\n","\n","c1, c2: L1 and L2 regularization to prevent overfitting\n","\n","all_possible_transitions=True: Helps generalization on unseen transitions\n","\n","The model is trained using crf.fit(X_train, y_train).\n","\n","4. Evaluation\n","Accuracy is calculated using flat_accuracy_score(), which evaluates word-level tag accuracy across all sentences.\n","\n","5. Tagging New Sentences\n","Function tag_new_sentence():\n","\n","Takes a raw sentence, converts it into a dummy tagged format (with fake 'NN' tags),\n","\n","Extracts features and predicts tags using the trained CRF model,\n","\n","Returns word-tag pairs.\n","\n"],"metadata":{"id":"ppYwHmOMZWXx"}},{"cell_type":"code","source":["import nltk\n","pip install sklearn-crfsuite\n","import sklearn_crfsuite\n","from sklearn_crfsuite import metrics\n","from nltk.corpus import treebank\n","nltk.download('treebank')\n","\n","# Prepare data\n","sentences = treebank.tagged_sents()\n","train_sents = sentences[:3000]\n","test_sents = sentences[3000:]\n","\n","# Feature extractor for each word\n","def word2features(sent, i):\n","    word = sent[i][0]\n","    features = {\n","        'bias': 1.0,\n","        'word.lower()': word.lower(),\n","        'word[-3:]': word[-3:],\n","        'word[-2:]': word[-2:],\n","        'word.isupper()': word.isupper(),\n","        'word.istitle()': word.istitle(),\n","        'word.isdigit()': word.isdigit(),\n","    }\n","    if i > 0:\n","        word1 = sent[i-1][0]\n","        features.update({\n","            '-1:word.lower()': word1.lower(),\n","            '-1:word.istitle()': word1.istitle(),\n","            '-1:word.isupper()': word1.isupper(),\n","        })\n","    else:\n","        features['BOS'] = True  # Beginning of sentence\n","\n","    if i < len(sent)-1:\n","        word1 = sent[i+1][0]\n","        features.update({\n","            '+1:word.lower()': word1.lower(),\n","            '+1:word.istitle()': word1.istitle(),\n","            '+1:word.isupper()': word1.isupper(),\n","        })\n","    else:\n","        features['EOS'] = True  # End of sentence\n","\n","    return features\n","\n","def sent2features(sent):\n","    return [word2features(sent, i) for i in range(len(sent))]\n","\n","def sent2labels(sent):\n","    return [label for (token, label) in sent]\n","\n","def sent2tokens(sent):\n","    return [token for (token, label) in sent]\n","\n","# Feature transformation\n","X_train = [sent2features(s) for s in train_sents]\n","y_train = [sent2labels(s) for s in train_sents]\n","\n","X_test = [sent2features(s) for s in test_sents]\n","y_test = [sent2labels(s) for s in test_sents]\n","\n","# Train CRF model\n","crf = sklearn_crfsuite.CRF(\n","    algorithm='lbfgs',\n","    c1=0.1,\n","    c2=0.1,\n","    max_iterations=100,\n","    all_possible_transitions=True,\n",")\n","crf.fit(X_train, y_train)\n","\n","# Evaluation\n","y_pred = crf.predict(X_test)\n","accuracy = metrics.flat_accuracy_score(y_test, y_pred)\n","print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n","\n","# POS tag a new sentence\n","def tag_new_sentence(crf, sentence):\n","    dummy_sent = [(word, 'NN') for word in sentence.split()]\n","    features = sent2features(dummy_sent)\n","    tags = crf.predict([features])[0]\n","    return list(zip(sentence.split(), tags))\n","\n","# Example usage\n","print(\"Tagged:\", tag_new_sentence(crf, \"The quick brown fox jumps over the lazy dog\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rmgbTiYZYEFc","executionInfo":{"status":"ok","timestamp":1746115015343,"user_tz":-330,"elapsed":73917,"user":{"displayName":"Jiya Gayawer","userId":"05781935709705850764"}},"outputId":"ad3ca17c-89e6-49b4-fac3-710ad57d0b94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Package treebank is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Accuracy on test set: 95.67%\n","Tagged: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'NNS'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wzRoR78qYr1P"},"execution_count":null,"outputs":[]}]}